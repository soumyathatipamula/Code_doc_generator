{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import RobertaForMaskedLM, Trainer, TrainingArguments\n",
    "import torch\n",
    "from transformers import DataCollatorWithPadding\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HF_DATASETS_CACHE'] = './Datasets'\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO']='0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'> {'repository_name': 'ageitgey/face_recognition', 'func_path_in_repository': 'examples/face_recognition_knn.py', 'func_name': 'predict', 'whole_func_string': 'def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):\\n    \"\"\"\\n    Recognizes faces in given image using a trained KNN classifier\\n\\n    :param X_img_path: path to image to be recognized\\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\\n    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\\n    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\\n           of mis-classifying an unknown person as a known one.\\n    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\\n        For faces of unrecognized persons, the name \\'unknown\\' will be returned.\\n    \"\"\"\\n    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:\\n        raise Exception(\"Invalid image path: {}\".format(X_img_path))\\n\\n    if knn_clf is None and model_path is None:\\n        raise Exception(\"Must supply knn classifier either thourgh knn_clf or model_path\")\\n\\n    # Load a trained KNN model (if one was passed in)\\n    if knn_clf is None:\\n        with open(model_path, \\'rb\\') as f:\\n            knn_clf = pickle.load(f)\\n\\n    # Load image file and find face locations\\n    X_img = face_recognition.load_image_file(X_img_path)\\n    X_face_locations = face_recognition.face_locations(X_img)\\n\\n    # If no faces are found in the image, return an empty result.\\n    if len(X_face_locations) == 0:\\n        return []\\n\\n    # Find encodings for faces in the test iamge\\n    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations)\\n\\n    # Use the KNN model to find the best matches for the test face\\n    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)\\n    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]\\n\\n    # Predict classes and remove classifications that aren\\'t within the threshold\\n    return [(pred, loc) if rec else (\"unknown\", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]', 'language': 'python', 'func_code_string': 'def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):\\n    \"\"\"\\n    Recognizes faces in given image using a trained KNN classifier\\n\\n    :param X_img_path: path to image to be recognized\\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\\n    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\\n    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\\n           of mis-classifying an unknown person as a known one.\\n    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\\n        For faces of unrecognized persons, the name \\'unknown\\' will be returned.\\n    \"\"\"\\n    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:\\n        raise Exception(\"Invalid image path: {}\".format(X_img_path))\\n\\n    if knn_clf is None and model_path is None:\\n        raise Exception(\"Must supply knn classifier either thourgh knn_clf or model_path\")\\n\\n    # Load a trained KNN model (if one was passed in)\\n    if knn_clf is None:\\n        with open(model_path, \\'rb\\') as f:\\n            knn_clf = pickle.load(f)\\n\\n    # Load image file and find face locations\\n    X_img = face_recognition.load_image_file(X_img_path)\\n    X_face_locations = face_recognition.face_locations(X_img)\\n\\n    # If no faces are found in the image, return an empty result.\\n    if len(X_face_locations) == 0:\\n        return []\\n\\n    # Find encodings for faces in the test iamge\\n    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations)\\n\\n    # Use the KNN model to find the best matches for the test face\\n    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)\\n    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]\\n\\n    # Predict classes and remove classifications that aren\\'t within the threshold\\n    return [(pred, loc) if rec else (\"unknown\", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]', 'func_code_tokens': ['def', 'predict', '(', 'X_img_path', ',', 'knn_clf', '=', 'None', ',', 'model_path', '=', 'None', ',', 'distance_threshold', '=', '0.6', ')', ':', 'if', 'not', 'os', '.', 'path', '.', 'isfile', '(', 'X_img_path', ')', 'or', 'os', '.', 'path', '.', 'splitext', '(', 'X_img_path', ')', '[', '1', ']', '[', '1', ':', ']', 'not', 'in', 'ALLOWED_EXTENSIONS', ':', 'raise', 'Exception', '(', '\"Invalid image path: {}\"', '.', 'format', '(', 'X_img_path', ')', ')', 'if', 'knn_clf', 'is', 'None', 'and', 'model_path', 'is', 'None', ':', 'raise', 'Exception', '(', '\"Must supply knn classifier either thourgh knn_clf or model_path\"', ')', '# Load a trained KNN model (if one was passed in)', 'if', 'knn_clf', 'is', 'None', ':', 'with', 'open', '(', 'model_path', ',', \"'rb'\", ')', 'as', 'f', ':', 'knn_clf', '=', 'pickle', '.', 'load', '(', 'f', ')', '# Load image file and find face locations', 'X_img', '=', 'face_recognition', '.', 'load_image_file', '(', 'X_img_path', ')', 'X_face_locations', '=', 'face_recognition', '.', 'face_locations', '(', 'X_img', ')', '# If no faces are found in the image, return an empty result.', 'if', 'len', '(', 'X_face_locations', ')', '==', '0', ':', 'return', '[', ']', '# Find encodings for faces in the test iamge', 'faces_encodings', '=', 'face_recognition', '.', 'face_encodings', '(', 'X_img', ',', 'known_face_locations', '=', 'X_face_locations', ')', '# Use the KNN model to find the best matches for the test face', 'closest_distances', '=', 'knn_clf', '.', 'kneighbors', '(', 'faces_encodings', ',', 'n_neighbors', '=', '1', ')', 'are_matches', '=', '[', 'closest_distances', '[', '0', ']', '[', 'i', ']', '[', '0', ']', '<=', 'distance_threshold', 'for', 'i', 'in', 'range', '(', 'len', '(', 'X_face_locations', ')', ')', ']', \"# Predict classes and remove classifications that aren't within the threshold\", 'return', '[', '(', 'pred', ',', 'loc', ')', 'if', 'rec', 'else', '(', '\"unknown\"', ',', 'loc', ')', 'for', 'pred', ',', 'loc', ',', 'rec', 'in', 'zip', '(', 'knn_clf', '.', 'predict', '(', 'faces_encodings', ')', ',', 'X_face_locations', ',', 'are_matches', ')', ']'], 'func_documentation_string': \"Recognizes faces in given image using a trained KNN classifier\\n\\n    :param X_img_path: path to image to be recognized\\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\\n    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\\n    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\\n           of mis-classifying an unknown person as a known one.\\n    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\\n        For faces of unrecognized persons, the name 'unknown' will be returned.\", 'func_documentation_tokens': ['Recognizes', 'faces', 'in', 'given', 'image', 'using', 'a', 'trained', 'KNN', 'classifier'], 'split_name': 'train', 'func_code_url': 'https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L111-L150'}\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Load the CodeSearchNet datasets for Python and Java\n",
    "train_python = load_dataset('code_search_net', 'python', trust_remote_code=True, cache_dir='./Datasets', split='train')\n",
    "test_python = load_dataset('code_search_net', 'python', trust_remote_code=True, cache_dir='./Datasets', split='test')\n",
    "val_python = load_dataset('code_search_net', 'python', trust_remote_code=True, cache_dir='./Datasets', split='validation')\n",
    "\n",
    "# dataset_javascript = load_dataset('code_search_net', 'javascript', trust_remote_code=True)\n",
    "print(type(train_python), train_python[1])\n",
    "# Combine the training sets of the datasets for multiple programming language\n",
    "train_dataset = train_python.shuffle(seed=42).select(range(100))\n",
    "print(type(train_dataset))\n",
    "# combined_dataset = [combined_dataset, dataset_javascript['train']])\n",
    "\n",
    "test_dataset = test_python.shuffle(seed=42).select(range(100))\n",
    "\n",
    "# Sample for validation\n",
    "val_dataset = val_python.shuffle(seed=42).select(range(50))\n",
    "\n",
    "# Display the first example from the combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 11),\n",
       " (100, 11),\n",
       " (50, 11),\n",
       " datasets.arrow_dataset.Dataset,\n",
       " datasets.arrow_dataset.Dataset,\n",
       " datasets.arrow_dataset.Dataset,\n",
       " dict)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape,test_dataset.shape, val_dataset.shape, type(train_dataset), type(val_dataset), type(test_dataset), type(train_dataset[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:3274: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "token = \"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\"\n",
    "model = RobertaForMaskedLM.from_pretrained(\"microsoft/codebert-base\", use_auth_token=token, cache_dir = \"./Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda'), 'NVIDIA GeForce GTX 1650')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "device, torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for your model (e.g., CodeBERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['func_code_string'],  # Adjust the key based on your dataset structure\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_texts = [example['func_code_string'] for example in train_python]\n",
    "doc_texts = [example['func_documentation_string'] for example in train_python]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_code = tokenizer(code_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "encoded_docs = tokenizer(doc_texts, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=1000,\n",
    "    remove_unused_columns=False,\n",
    "    save_total_limit=2,\n",
    "    fp16=True # Set this to False to avoid the column check\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the DataLoader with the custom collate function\n",
    "train_dataloader = DataLoader(tokenized_train_data, batch_size=16, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[0, 9232, 2310, 1640, 13367, 43, 43839, 49460, 35, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 49434, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 47416, 2469, 7471, 37127, 4333, 8210, 48128, 18400, 37127, 15389, 9264, 36714, 4394, 9085, 48823, 711, 48569, 47504, 18164, 17, 48, 47240, 7487, 41907, 14292, 48, 17, 46, 43251, 4394, 14285, 48732, 4333, 47842, 15389, 48569, 48745, 5782, 47089, 9253, 37127, 14292, 5543, 44636, 23171, 37127, 10674, 9253, 45262, 711, 45682, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 47416, 12410, 10470, 49257, 48820, 15722, 47873, 11423, 46499, 12736, 318, 12, 29802, 12, 42633, 1589, 4799, 1073, 6, 47111, 10659, 14285, 318, 12, 30597, 3786, 12, 11321, 47111, 10659, 14285, 1426, 12, 30597, 3786, 1437, 49065, 49117, 20024, 44574, 36714, 4394, 9085, 48823, 711, 47658, 9357, 42393, 15722, 27819, 45682, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 47111, 48, 11423, 36484, 2840, 4726, 36714, 10809, 2840, 48105, 46015, 10278, 47876, 25448, 47983, 15113, 46015, 10278, 48145, 27, 46499, 3726, 42393, 9085, 8384, 48186, 10674, 47983, 20024, 47504, 12736, 48186, 10674, 48128, 8210, 48558, 15722, 47873, 11423, 47089, 862, 45682, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 49434, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 5448, 1215, 6031, 5457, 1403, 4, 45416, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 114, 5448, 1215, 6031, 49333, 128, 18760, 108, 8, 5448, 1215, 6031, 49333, 128, 36696, 13373, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 671, 35297, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 579, 5457, 1403, 4, 49575, 4, 29552, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 114, 36, 29, 49095, 1878, 8, 579, 28696, 2993, 43, 50, 579, 45994, 33121, 35, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 671, 2310, 1640, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1403, 4, 47288, 6, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 36, 13367, 4, 41510, 8, 1403, 4, 41510, 4, 47288, 43, 50, 25522, 48268, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 4839, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 671, 35297, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 9232, 15095, 8643, 1640, 13367, 3256, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 49434, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1292, 46917, 7510, 16763, 13, 10, 23956, 1300, 6, 25, 10, 28700, 4, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 49434, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 671, 28700, 48443, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 31509, 808, 3934, 7031, 1640, 13367, 4, 808, 46934, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 31509, 13650, 3934, 7031, 1640, 13367, 4, 13650, 46934, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 31509, 90, 9041, 10003, 43575, 3934, 7031, 1640, 13367, 4, 4328, 90, 46934, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 49670, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 9232, 2935, 1215, 39281, 1640, 13367, 3256, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 49434, 39962, 13076, 2728, 72, 48149, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 849, 3310, 5, 1150, 18, 5448, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 2422, 1640, 49577, 6, 1403, 322, 45061, 1215, 39281, 43048, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 849, 4287, 16047, 6296, 1635, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 849, 14718, 8, 7425, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1403, 4, 39281, 48759, 6199, 108, 46386, 108, 11127, 18620, 44403, 5457, 1403, 4, 6460, 1215, 39383, 1215, 12376, 1640, 13367, 4, 45579, 48759, 6199, 108, 7479, 4532, 5214, 13367, 4, 45579, 48759, 30033, 108, 45587, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[0, 37127, 2469, 7471, 37127, 4333, 8210, 48128, 18400, 37127, 15389, 9264, 36714, 4394, 9085, 48823, 711, 48569, 47504, 18164, 17, 48, 47240, 7487, 41907, 14292, 48, 17, 46, 43251, 4394, 14285, 48732, 4333, 47842, 15389, 48569, 48745, 5782, 47089, 9253, 37127, 14292, 5543, 44636, 23171, 37127, 10674, 9253, 45262, 711, 45682, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 47416, 12410, 10470, 49257, 48820, 15722, 47873, 11423, 46499, 12736, 318, 12, 29802, 12, 42633, 1589, 4799, 1073, 6, 47111, 10659, 14285, 318, 12, 30597, 3786, 12, 11321, 47111, 10659, 14285, 1426, 12, 30597, 3786, 1437, 49065, 49117, 20024, 44574, 36714, 4394, 9085, 48823, 711, 47658, 9357, 42393, 15722, 27819, 45682, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 47111, 48, 11423, 36484, 2840, 4726, 36714, 10809, 2840, 48105, 46015, 10278, 47876, 25448, 47983, 15113, 46015, 10278, 48145, 27, 46499, 3726, 42393, 9085, 8384, 48186, 10674, 47983, 20024, 47504, 12736, 48186, 10674, 48128, 8210, 48558, 15722, 47873, 11423, 47089, 862, 45682, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 9344, 46917, 7510, 16763, 13, 10, 23956, 1300, 6, 25, 10, 28700, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 39962, 13076, 2728, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# Check the first few examples in your tokenized data\n",
    "print(tokenized_train_data[:3])  # Adjust slicing based on your data structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\accelerator.py:463: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Use this Dataset with the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,  # Your pre-initialized model\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=train_dataset,  # Use the custom Dataset\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator  # Optional evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2345\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2342\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2344\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2345\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_batched_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_num_input_tokens_seen\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\data_loader.py:452\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 452\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[1;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[1;32mc:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3453\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   3451\u001b[0m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has been passed for padding\u001b[39;00m\n\u001b[0;32m   3452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[1;32m-> 3453\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3454\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3455\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3456\u001b[0m     )\n\u001b[0;32m   3458\u001b[0m required_input \u001b[38;5;241m=\u001b[39m encoded_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m   3460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url']"
     ]
    }
   ],
   "source": [
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### existing code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "    num_rows: 100\n",
      "})\n",
      "{'repository_name': 'zeromake/aiko', 'func_path_in_repository': 'aiko/request.py', 'func_name': 'Request.fresh', 'whole_func_string': 'def fresh(self) -> bool:\\n        \"\"\"\\n        检查请求缓存是否“新鲜”，也就是内容没有改变。\\n        此方法用于 If-None-Match / ETag, 和 If-Modified-Since 和 Last-Modified 之间的缓存协商。\\n        在设置一个或多个这些响应头后应该引用它。\\n        \"\"\"\\n        method_str = self.method\\n        if method_str != \\'GET\\' and method_str != \\'HEAD\\':\\n            return False\\n        s = self.ctx.status\\n        if (s >= 200 and s < 300) or s == 304:\\n            return fresh(\\n                self.headers,\\n                (self.response and self.response.headers) or {},\\n            )\\n        return False', 'language': 'python', 'func_code_string': 'def fresh(self) -> bool:\\n        \"\"\"\\n        检查请求缓存是否“新鲜”，也就是内容没有改变。\\n        此方法用于 If-None-Match / ETag, 和 If-Modified-Since 和 Last-Modified 之间的缓存协商。\\n        在设置一个或多个这些响应头后应该引用它。\\n        \"\"\"\\n        method_str = self.method\\n        if method_str != \\'GET\\' and method_str != \\'HEAD\\':\\n            return False\\n        s = self.ctx.status\\n        if (s >= 200 and s < 300) or s == 304:\\n            return fresh(\\n                self.headers,\\n                (self.response and self.response.headers) or {},\\n            )\\n        return False', 'func_code_tokens': ['def', 'fresh', '(', 'self', ')', '->', 'bool', ':', 'method_str', '=', 'self', '.', 'method', 'if', 'method_str', '!=', \"'GET'\", 'and', 'method_str', '!=', \"'HEAD'\", ':', 'return', 'False', 's', '=', 'self', '.', 'ctx', '.', 'status', 'if', '(', 's', '>=', '200', 'and', 's', '<', '300', ')', 'or', 's', '==', '304', ':', 'return', 'fresh', '(', 'self', '.', 'headers', ',', '(', 'self', '.', 'response', 'and', 'self', '.', 'response', '.', 'headers', ')', 'or', '{', '}', ',', ')', 'return', 'False'], 'func_documentation_string': '检查请求缓存是否“新鲜”，也就是内容没有改变。\\n        此方法用于 If-None-Match / ETag, 和 If-Modified-Since 和 Last-Modified 之间的缓存协商。\\n        在设置一个或多个这些响应头后应该引用它。', 'func_documentation_tokens': ['检查请求缓存是否“新鲜”，也就是内容没有改变。', '此方法用于', 'If', '-', 'None', '-', 'Match', '/', 'ETag', '和', 'If', '-', 'Modified', '-', 'Since', '和', 'Last', '-', 'Modified', '之间的缓存协商。', '在设置一个或多个这些响应头后应该引用它。'], 'split_name': 'train', 'func_code_url': 'https://github.com/zeromake/aiko/blob/53b246fa88652466a9e38ac3d1a99a6198195b0f/aiko/request.py#L567-L582'}\n",
      "Dataset({\n",
      "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 100\n",
      "})\n",
      "{'repository_name': 'zeromake/aiko', 'func_path_in_repository': 'aiko/request.py', 'func_name': 'Request.fresh', 'whole_func_string': 'def fresh(self) -> bool:\\n        \"\"\"\\n        检查请求缓存是否“新鲜”，也就是内容没有改变。\\n        此方法用于 If-None-Match / ETag, 和 If-Modified-Since 和 Last-Modified 之间的缓存协商。\\n        在设置一个或多个这些响应头后应该引用它。\\n        \"\"\"\\n        method_str = self.method\\n        if method_str != \\'GET\\' and method_str != \\'HEAD\\':\\n            return False\\n        s = self.ctx.status\\n        if (s >= 200 and s < 300) or s == 304:\\n            return fresh(\\n                self.headers,\\n                (self.response and self.response.headers) or {},\\n            )\\n        return False', 'language': 'python', 'func_code_string': 'def fresh(self) -> bool:\\n        \"\"\"\\n        检查请求缓存是否“新鲜”，也就是内容没有改变。\\n        此方法用于 If-None-Match / ETag, 和 If-Modified-Since 和 Last-Modified 之间的缓存协商。\\n        在设置一个或多个这些响应头后应该引用它。\\n        \"\"\"\\n        method_str = self.method\\n        if method_str != \\'GET\\' and method_str != \\'HEAD\\':\\n            return False\\n        s = self.ctx.status\\n        if (s >= 200 and s < 300) or s == 304:\\n            return fresh(\\n                self.headers,\\n                (self.response and self.response.headers) or {},\\n            )\\n        return False', 'func_code_tokens': ['def', 'fresh', '(', 'self', ')', '->', 'bool', ':', 'method_str', '=', 'self', '.', 'method', 'if', 'method_str', '!=', \"'GET'\", 'and', 'method_str', '!=', \"'HEAD'\", ':', 'return', 'False', 's', '=', 'self', '.', 'ctx', '.', 'status', 'if', '(', 's', '>=', '200', 'and', 's', '<', '300', ')', 'or', 's', '==', '304', ':', 'return', 'fresh', '(', 'self', '.', 'headers', ',', '(', 'self', '.', 'response', 'and', 'self', '.', 'response', '.', 'headers', ')', 'or', '{', '}', ',', ')', 'return', 'False'], 'func_documentation_string': '检查请求缓存是否“新鲜”，也就是内容没有改变。\\n        此方法用于 If-None-Match / ETag, 和 If-Modified-Since 和 Last-Modified 之间的缓存协商。\\n        在设置一个或多个这些响应头后应该引用它。', 'func_documentation_tokens': ['检查请求缓存是否“新鲜”，也就是内容没有改变。', '此方法用于', 'If', '-', 'None', '-', 'Match', '/', 'ETag', '和', 'If', '-', 'Modified', '-', 'Since', '和', 'Last', '-', 'Modified', '之间的缓存协商。', '在设置一个或多个这些响应头后应该引用它。'], 'split_name': 'train', 'func_code_url': 'https://github.com/zeromake/aiko/blob/53b246fa88652466a9e38ac3d1a99a6198195b0f/aiko/request.py#L567-L582', 'input_ids': [0, 9232, 2310, 1640, 13367, 43, 43839, 49460, 35, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 49434, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 47416, 2469, 7471, 37127, 4333, 8210, 48128, 18400, 37127, 15389, 9264, 36714, 4394, 9085, 48823, 711, 48569, 47504, 18164, 17, 48, 47240, 7487, 41907, 14292, 48, 17, 46, 43251, 4394, 14285, 48732, 4333, 47842, 15389, 48569, 48745, 5782, 47089, 9253, 37127, 14292, 5543, 44636, 23171, 37127, 10674, 9253, 45262, 711, 45682, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 47416, 12410, 10470, 49257, 48820, 15722, 47873, 11423, 46499, 12736, 318, 12, 29802, 12, 42633, 1589, 4799, 1073, 6, 47111, 10659, 14285, 318, 12, 30597, 3786, 12, 11321, 47111, 10659, 14285, 1426, 12, 30597, 3786, 1437, 49065, 49117, 20024, 44574, 36714, 4394, 9085, 48823, 711, 47658, 9357, 42393, 15722, 27819, 45682, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 47111, 48, 11423, 36484, 2840, 4726, 36714, 10809, 2840, 48105, 46015, 10278, 47876, 25448, 47983, 15113, 46015, 10278, 48145, 27, 46499, 3726, 42393, 9085, 8384, 48186, 10674, 47983, 20024, 47504, 12736, 48186, 10674, 48128, 8210, 48558, 15722, 47873, 11423, 47089, 862, 45682, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 49434, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 5448, 1215, 6031, 5457, 1403, 4, 45416, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 114, 5448, 1215, 6031, 49333, 128, 18760, 108, 8, 5448, 1215, 6031, 49333, 128, 36696, 13373, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 671, 35297, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 579, 5457, 1403, 4, 49575, 4, 29552, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 114, 36, 29, 49095, 1878, 8, 579, 28696, 2993, 43, 50, 579, 45994, 33121, 35, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 671, 2310, 1640, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1403, 4, 47288, 6, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 36, 13367, 4, 41510, 8, 1403, 4, 41510, 4, 47288, 43, 50, 25522, 48268, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 4839, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 671, 35297, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0, 37127, 2469, 7471, 37127, 4333, 8210, 48128, 18400, 37127, 15389, 9264, 36714, 4394, 9085, 48823, 711, 48569, 47504, 18164, 17, 48, 47240, 7487, 41907, 14292, 48, 17, 46, 43251, 4394, 14285, 48732, 4333, 47842, 15389, 48569, 48745, 5782, 47089, 9253, 37127, 14292, 5543, 44636, 23171, 37127, 10674, 9253, 45262, 711, 45682, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 47416, 12410, 10470, 49257, 48820, 15722, 47873, 11423, 46499, 12736, 318, 12, 29802, 12, 42633, 1589, 4799, 1073, 6, 47111, 10659, 14285, 318, 12, 30597, 3786, 12, 11321, 47111, 10659, 14285, 1426, 12, 30597, 3786, 1437, 49065, 49117, 20024, 44574, 36714, 4394, 9085, 48823, 711, 47658, 9357, 42393, 15722, 27819, 45682, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 47111, 48, 11423, 36484, 2840, 4726, 36714, 10809, 2840, 48105, 46015, 10278, 47876, 25448, 47983, 15113, 46015, 10278, 48145, 27, 46499, 3726, 42393, 9085, 8384, 48186, 10674, 47983, 20024, 47504, 12736, 48186, 10674, 48128, 8210, 48558, 15722, 47873, 11423, 47089, 862, 45682, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 100\n",
      "})\n",
      "{'input_ids': [0, 9232, 2310, 1640, 13367, 43, 43839, 49460, 35, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 49434, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 47416, 2469, 7471, 37127, 4333, 8210, 48128, 18400, 37127, 15389, 9264, 36714, 4394, 9085, 48823, 711, 48569, 47504, 18164, 17, 48, 47240, 7487, 41907, 14292, 48, 17, 46, 43251, 4394, 14285, 48732, 4333, 47842, 15389, 48569, 48745, 5782, 47089, 9253, 37127, 14292, 5543, 44636, 23171, 37127, 10674, 9253, 45262, 711, 45682, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 47416, 12410, 10470, 49257, 48820, 15722, 47873, 11423, 46499, 12736, 318, 12, 29802, 12, 42633, 1589, 4799, 1073, 6, 47111, 10659, 14285, 318, 12, 30597, 3786, 12, 11321, 47111, 10659, 14285, 1426, 12, 30597, 3786, 1437, 49065, 49117, 20024, 44574, 36714, 4394, 9085, 48823, 711, 47658, 9357, 42393, 15722, 27819, 45682, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 47111, 48, 11423, 36484, 2840, 4726, 36714, 10809, 2840, 48105, 46015, 10278, 47876, 25448, 47983, 15113, 46015, 10278, 48145, 27, 46499, 3726, 42393, 9085, 8384, 48186, 10674, 47983, 20024, 47504, 12736, 48186, 10674, 48128, 8210, 48558, 15722, 47873, 11423, 47089, 862, 45682, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 49434, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 5448, 1215, 6031, 5457, 1403, 4, 45416, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 114, 5448, 1215, 6031, 49333, 128, 18760, 108, 8, 5448, 1215, 6031, 49333, 128, 36696, 13373, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 671, 35297, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 579, 5457, 1403, 4, 49575, 4, 29552, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 114, 36, 29, 49095, 1878, 8, 579, 28696, 2993, 43, 50, 579, 45994, 33121, 35, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 671, 2310, 1640, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1403, 4, 47288, 6, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 36, 13367, 4, 41510, 8, 1403, 4, 41510, 4, 47288, 43, 50, 25522, 48268, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 4839, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 671, 35297, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0, 37127, 2469, 7471, 37127, 4333, 8210, 48128, 18400, 37127, 15389, 9264, 36714, 4394, 9085, 48823, 711, 48569, 47504, 18164, 17, 48, 47240, 7487, 41907, 14292, 48, 17, 46, 43251, 4394, 14285, 48732, 4333, 47842, 15389, 48569, 48745, 5782, 47089, 9253, 37127, 14292, 5543, 44636, 23171, 37127, 10674, 9253, 45262, 711, 45682, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 47416, 12410, 10470, 49257, 48820, 15722, 47873, 11423, 46499, 12736, 318, 12, 29802, 12, 42633, 1589, 4799, 1073, 6, 47111, 10659, 14285, 318, 12, 30597, 3786, 12, 11321, 47111, 10659, 14285, 1426, 12, 30597, 3786, 1437, 49065, 49117, 20024, 44574, 36714, 4394, 9085, 48823, 711, 47658, 9357, 42393, 15722, 27819, 45682, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 47111, 48, 11423, 36484, 2840, 4726, 36714, 10809, 2840, 48105, 46015, 10278, 47876, 25448, 47983, 15113, 46015, 10278, 48145, 27, 46499, 3726, 42393, 9085, 8384, 48186, 10674, 47983, 20024, 47504, 12736, 48186, 10674, 48128, 8210, 48558, 15722, 47873, 11423, 47089, 862, 45682, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the datasets\n",
    "print(train_dataset)\n",
    "print(train_dataset[0])\n",
    "tokenized_train_data = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_data = val_dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_train_data)\n",
    "print(tokenized_train_data[0])\n",
    "# Remove unused columns if necessary\n",
    "tokenized_train_data = tokenized_train_data.remove_columns(train_dataset.column_names)\n",
    "tokenized_val_data = tokenized_val_data.remove_columns(val_dataset.column_names)\n",
    "print(tokenized_train_data)\n",
    "print(tokenized_train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.data = tokenized_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Wrap your tokenized data into a Dataset\n",
    "#train_dataset = CustomDataset(tokenized_train_data)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Extract the necessary components from each item in the batch\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Convert lists of lists into tensors\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids).to(device),\n",
    "        'attention_mask': torch.tensor(attention_mask).to(device),\n",
    "        'labels': torch.tensor(labels).to(device),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    input_encodings = tokenizer(\n",
    "        examples['func_code_string'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        examples['func_documentation_string'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_encodings['input_ids'],  # Should be list of integers\n",
    "        'attention_mask': input_encodings['attention_mask'],  # Should be list of integers\n",
    "        'labels': labels['input_ids'],  # Should be list of integers\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 866629/866629 [03:34<00:00, 4035.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the datasets\n",
    "tokenized_train_data = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_data = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove the original code strings if necessary\n",
    "tokenized_train_data = tokenized_train_data.remove_columns(['func_code_string'])\n",
    "tokenized_val_data = tokenized_val_data.remove_columns(['func_code_string'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the input code and the corresponding documentation\n",
    "    input_encodings = tokenizer(\n",
    "        examples['func_code_string'],  # Adjust the key based on your dataset structure\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        examples['func_documentation_string'],  # Adjust this to the correct documentation key\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    # Return input encodings and labels\n",
    "    return {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': labels['input_ids'],  # This is how the model expects labels\n",
    "    }\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train_data = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_data = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove unused columns if necessary\n",
    "tokenized_train_data = tokenized_train_data.remove_columns(train_dataset.column_names)\n",
    "tokenized_val_data = tokenized_val_data.remove_columns(val_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Assuming all inputs are lists of same length\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids),\n",
    "        'attention_mask': torch.tensor(attention_mask),\n",
    "        'labels': torch.tensor(labels),\n",
    "    }\n",
    "\n",
    "# Use this in your DataLoader\n",
    "train_dataloader = DataLoader(tokenized_train_data, batch_size=16, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_python['train'].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "token = \"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\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-multi\", use_auth_token=token, cache_dir = \"./Models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Tokenize the datasets using the correct field name\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['func_code_string'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = combined_dataset.map(tokenize_function, batched=True)\n",
    "# After tokenizing, save the dataset to a specific location\n",
    "output_directory = './Datasets/tokenized'\n",
    "tokenized_datasets.save_to_disk(output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a preprocessing function to tokenize your dataset\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['input_text']  # Replace with the correct key for input texts\n",
    "    targets = examples['target_text']  # Replace with the correct key for target texts\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "    \n",
    "    # Tokenize the targets (for seq2seq models like T5)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=512, truncation=True)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing to your dataset\n",
    "tokenized_datasets = combined_dataset.map(tokenize_function, batched=True)\n",
    "# After tokenizing, save the dataset to a specific location\n",
    "output_directory = './Datasets/tokenized'\n",
    "tokenized_datasets.save_to_disk(output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization\n",
    "tokenized_datasets_validation = validation_dataset.map(tokenize_function, batched=True)\n",
    "# After tokenizing, save the dataset to a specific location\n",
    "output_directory = './Datasets/tokenized/validation'\n",
    "tokenized_datasets_validation.save_to_disk(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSeq2SeqLM\n",
    "# Load your pre-trained model (e.g., T5 or CodeBERT)\n",
    "model_name = \"t5-base\"  # or another model suitable for code-to-text tasks\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                  # output directory for model predictions and checkpoints\n",
    "    evaluation_strategy=\"steps\",             # evaluation is done at the end of each epoch\n",
    "    learning_rate=5e-5,                      # learning rate\n",
    "    per_device_train_batch_size=3,           # batch size for training\n",
    "    per_device_eval_batch_size=3,            # batch size for evaluation\n",
    "    num_train_epochs=3,                      # total number of training epochs\n",
    "    weight_decay=0.01,                       # strength of weight decay\n",
    "    logging_dir='./logs',                    # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,             # load the best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\"        # metric for determining the best model\n",
    ")\n",
    "\n",
    "# Function to compute metrics\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions.argmax(axis=-1)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels_ids = np.where(labels_ids != -100, labels_ids, tokenizer.pad_token_id)\n",
    "    # Decode the predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Use a metric such as BLEU or ROUGE here\n",
    "    # For simplicity, you can use the following placeholder:\n",
    "    return {\"bleu\": calculate_bleu(decoded_preds, decoded_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets)  # to check the overall structure\n",
    "print(tokenized_datasets.column_names)  # to list all column names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")  # Change to \"cuda\" when you want to run on GPU\n",
    "#model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# Custom data collator\n",
    "def custom_data_collator(features):\n",
    "    if not isinstance(features[0], dict):\n",
    "        # Ensure that features are converted to dicts\n",
    "        features = [vars(f) if hasattr(f, '__dict__') else f for f in features]\n",
    "    return DataCollatorForSeq2Seq(tokenizer)(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"func_code_string\"],\n",
    "    eval_dataset=tokenized_datasets_validation[\"func_code_string\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=custom_data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Tokenization function\n",
    "def prepare_data(example):\n",
    "    # Tokenizing code and documentation\n",
    "    code_encoding = tokenizer(example['func_code_tokens'], truncation=True, padding='max_length', max_length=512)\n",
    "    doc_encoding = tokenizer(example['func_documentation_tokens'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "    return {\n",
    "        'input_ids': code_encoding['input_ids'],  # Tokenized function code\n",
    "        'attention_mask': code_encoding['attention_mask'],  # Attention mask\n",
    "        'labels': doc_encoding['input_ids']  # Tokenized documentation\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
