{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"code_search_net\", \"python\",cache_dir=\"./Datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['repository_name',\n",
       "  'func_path_in_repository',\n",
       "  'func_name',\n",
       "  'whole_func_string',\n",
       "  'language',\n",
       "  'func_code_string',\n",
       "  'func_code_tokens',\n",
       "  'func_documentation_string',\n",
       "  'func_documentation_tokens',\n",
       "  'split_name',\n",
       "  'func_code_url'],\n",
       " 'test': ['repository_name',\n",
       "  'func_path_in_repository',\n",
       "  'func_name',\n",
       "  'whole_func_string',\n",
       "  'language',\n",
       "  'func_code_string',\n",
       "  'func_code_tokens',\n",
       "  'func_documentation_string',\n",
       "  'func_documentation_tokens',\n",
       "  'split_name',\n",
       "  'func_code_url'],\n",
       " 'validation': ['repository_name',\n",
       "  'func_path_in_repository',\n",
       "  'func_name',\n",
       "  'whole_func_string',\n",
       "  'language',\n",
       "  'func_code_string',\n",
       "  'func_code_tokens',\n",
       "  'func_documentation_string',\n",
       "  'func_documentation_tokens',\n",
       "  'split_name',\n",
       "  'func_code_url']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\", cache_dir=\"./Models\")\n",
    "model = RobertaForMaskedLM.from_pretrained(\"microsoft/codebert-base\", cache_dir=\"./Models\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_code(code_snippet, tokenizer):\n",
    "    return tokenizer(\n",
    "        code_snippet, max_length=512, truncation=True, padding='max_length', return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(batch, tokenizer):\n",
    "    batch[\"input_ids\"] = tokenize_code(batch[\"func_code_string\"], tokenizer)[\"input_ids\"]\n",
    "    batch[\"labels\"] = tokenize_code(batch[\"func_documentation_string\"], tokenizer)[\"input_ids\"]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(1000)).map(lambda x: preprocess_data(x, tokenizer), batched=True)\n",
    "eval_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(200)).map(lambda x: preprocess_data(x, tokenizer), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./codebert-finetuned\",    # output directory\n",
    "    num_train_epochs=3,                   # number of epochs\n",
    "    per_device_train_batch_size=16,       # batch size\n",
    "    per_device_eval_batch_size=16,        # eval batch size\n",
    "    warmup_steps=500,                     # warmup steps\n",
    "    weight_decay=0.01,                    # weight decay\n",
    "    logging_dir=\"./logs\",                 # logging directory\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",          # Evaluation after every logging step\n",
    "    save_total_limit=2,                   # Keep only last two checkpoints\n",
    "    save_steps=500,                       # Save model every 500 steps\n",
    "    report_to=\"none\",\n",
    "    fp16=True                     # No reports (e.g., to wandb)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\accelerator.py:463: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/189 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "c:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:370: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "  5%|â–Œ         | 10/189 [04:44<1:24:23, 28.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 18.0067, 'grad_norm': 62.93367385864258, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  5%|â–Œ         | 10/189 [06:35<1:24:23, 28.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 17.82748794555664, 'eval_runtime': 110.4363, 'eval_samples_per_second': 1.811, 'eval_steps_per_second': 0.118, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 20/189 [11:19<1:24:03, 29.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 17.4566, 'grad_norm': 55.4626579284668, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 11%|â–ˆ         | 20/189 [13:05<1:24:03, 29.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 17.223665237426758, 'eval_runtime': 105.609, 'eval_samples_per_second': 1.894, 'eval_steps_per_second': 0.123, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 30/189 [17:47<1:18:11, 29.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 16.6815, 'grad_norm': 49.75236129760742, 'learning_rate': 2.6e-06, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 16%|â–ˆâ–Œ        | 30/189 [19:34<1:18:11, 29.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 16.183364868164062, 'eval_runtime': 106.9426, 'eval_samples_per_second': 1.87, 'eval_steps_per_second': 0.122, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|â–ˆâ–ˆ        | 40/189 [24:16<1:12:59, 29.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 15.336, 'grad_norm': 48.57144546508789, 'learning_rate': 3.6e-06, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 21%|â–ˆâ–ˆ        | 40/189 [26:01<1:12:59, 29.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 14.338003158569336, 'eval_runtime': 104.5645, 'eval_samples_per_second': 1.913, 'eval_steps_per_second': 0.124, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–‹       | 50/189 [30:41<1:07:53, 29.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13.4126, 'grad_norm': 45.878501892089844, 'learning_rate': 4.6e-06, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 26%|â–ˆâ–ˆâ–‹       | 50/189 [32:27<1:07:53, 29.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 12.026623725891113, 'eval_runtime': 106.3166, 'eval_samples_per_second': 1.881, 'eval_steps_per_second': 0.122, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 60/189 [37:08<1:02:51, 29.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.7985, 'grad_norm': 40.57843780517578, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 60/189 [38:53<1:02:51, 29.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.35397720336914, 'eval_runtime': 105.5608, 'eval_samples_per_second': 1.895, 'eval_steps_per_second': 0.123, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 70/189 [43:21<57:33, 29.02s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.8631, 'grad_norm': 36.99539566040039, 'learning_rate': 6.6e-06, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 70/189 [45:06<57:33, 29.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.722663402557373, 'eval_runtime': 105.1723, 'eval_samples_per_second': 1.902, 'eval_steps_per_second': 0.124, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 80/189 [49:48<53:50, 29.63s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.2735, 'grad_norm': 33.96503829956055, 'learning_rate': 7.6e-06, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 80/189 [51:35<53:50, 29.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.225109577178955, 'eval_runtime': 106.6414, 'eval_samples_per_second': 1.875, 'eval_steps_per_second': 0.122, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 90/189 [57:00<1:06:47, 40.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0534, 'grad_norm': 14.488544464111328, 'learning_rate': 8.599999999999999e-06, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 90/189 [1:00:20<1:06:47, 40.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.609644889831543, 'eval_runtime': 200.4419, 'eval_samples_per_second': 0.998, 'eval_steps_per_second': 0.065, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 100/189 [1:08:46<1:18:43, 53.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8573, 'grad_norm': 6.039247989654541, 'learning_rate': 9.600000000000001e-06, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 100/189 [1:11:58<1:18:43, 53.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1470143795013428, 'eval_runtime': 192.1054, 'eval_samples_per_second': 1.041, 'eval_steps_per_second': 0.068, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 110/189 [1:17:25<41:00, 31.15s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9605, 'grad_norm': 9.008535385131836, 'learning_rate': 1.06e-05, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 110/189 [1:19:09<41:00, 31.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9461963176727295, 'eval_runtime': 104.2092, 'eval_samples_per_second': 1.919, 'eval_steps_per_second': 0.125, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 120/189 [1:23:41<32:48, 28.53s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6761, 'grad_norm': 10.840803146362305, 'learning_rate': 1.16e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 120/189 [1:25:24<32:48, 28.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9073281288146973, 'eval_runtime': 102.8442, 'eval_samples_per_second': 1.945, 'eval_steps_per_second': 0.126, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 130/189 [1:29:43<27:04, 27.54s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.505, 'grad_norm': 13.738783836364746, 'learning_rate': 1.2600000000000001e-05, 'epoch': 2.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 130/189 [1:31:25<27:04, 27.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7985397577285767, 'eval_runtime': 102.5091, 'eval_samples_per_second': 1.951, 'eval_steps_per_second': 0.127, 'epoch': 2.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/189 [1:35:57<23:12, 28.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4622, 'grad_norm': 3.4580392837524414, 'learning_rate': 1.3600000000000002e-05, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/189 [1:37:39<23:12, 28.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6921563148498535, 'eval_runtime': 102.2037, 'eval_samples_per_second': 1.957, 'eval_steps_per_second': 0.127, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 150/189 [1:42:15<18:52, 29.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3532, 'grad_norm': 3.9545507431030273, 'learning_rate': 1.4599999999999999e-05, 'epoch': 2.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 150/189 [1:43:58<18:52, 29.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6555218696594238, 'eval_runtime': 102.867, 'eval_samples_per_second': 1.944, 'eval_steps_per_second': 0.126, 'epoch': 2.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 160/189 [1:48:29<13:43, 28.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3292, 'grad_norm': 6.181820392608643, 'learning_rate': 1.56e-05, 'epoch': 2.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 160/189 [1:50:11<13:43, 28.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5807119607925415, 'eval_runtime': 102.1535, 'eval_samples_per_second': 1.958, 'eval_steps_per_second': 0.127, 'epoch': 2.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 170/189 [1:54:42<08:58, 28.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1491, 'grad_norm': 2.728649377822876, 'learning_rate': 1.66e-05, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 170/189 [1:56:24<08:58, 28.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5609850883483887, 'eval_runtime': 102.2638, 'eval_samples_per_second': 1.956, 'eval_steps_per_second': 0.127, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 180/189 [2:00:55<04:15, 28.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1541, 'grad_norm': 3.3209993839263916, 'learning_rate': 1.76e-05, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 180/189 [2:02:37<04:15, 28.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5190330743789673, 'eval_runtime': 102.3456, 'eval_samples_per_second': 1.954, 'eval_steps_per_second': 0.127, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [2:06:30<00:00, 40.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 7590.0833, 'train_samples_per_second': 0.395, 'train_steps_per_second': 0.025, 'train_loss': 6.4713023755916215, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=189, training_loss=6.4713023755916215, metrics={'train_runtime': 7590.0833, 'train_samples_per_second': 0.395, 'train_steps_per_second': 0.025, 'total_flos': 789796389888000.0, 'train_loss': 6.4713023755916215, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./codebert-finetuned\\\\tokenizer_config.json',\n",
       " './codebert-finetuned\\\\special_tokens_map.json',\n",
       " './codebert-finetuned\\\\vocab.json',\n",
       " './codebert-finetuned\\\\merges.txt',\n",
       " './codebert-finetuned\\\\added_tokens.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./codebert-finetuned\")\n",
    "tokenizer.save_pretrained(\"./codebert-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_documentation(code_snippet, model, tokenizer):\n",
    "    inputs = tokenizer(code_snippet, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    print(inputs)\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=150, num_beams=5, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_example = \"\"\"\n",
    "def add_numbers(a, b):\n",
    "    return a + b\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The current model class (RobertaForMaskedLM) is not compatible with `.generate()`, as it doesn't have a language model head. Classes that support generation often end in one of these names: ['ForCausalLM', 'ForConditionalGeneration', 'ForSpeechSeq2Seq', 'ForVision2Seq'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m documentation \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_documentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode_example\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Documentation:\u001b[39m\u001b[38;5;124m\"\u001b[39m, documentation)\n",
      "Cell \u001b[1;32mIn[34], line 3\u001b[0m, in \u001b[0;36mgenerate_documentation\u001b[1;34m(code_snippet, model, tokenizer)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_documentation\u001b[39m(code_snippet, model, tokenizer):\n\u001b[0;32m      2\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(code_snippet, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 3\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1801\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1718\u001b[0m \n\u001b[0;32m   1719\u001b[0m \u001b[38;5;124;03mGenerates sequences of token ids for models with a language modeling head.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1798\u001b[0m \u001b[38;5;124;03m            - [`~generation.GenerateBeamEncoderDecoderOutput`]\u001b[39;00m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1800\u001b[0m \u001b[38;5;66;03m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[39;00m\n\u001b[1;32m-> 1801\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1802\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Pull this out first, we only use it for stopping criteria\u001b[39;00m\n\u001b[0;32m   1803\u001b[0m generation_config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_generation_config(generation_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1125\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_class\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcan_generate():\n\u001b[0;32m   1119\u001b[0m     terminations_with_generation_support \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForCausalLM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForConditionalGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForSpeechSeq2Seq\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForVision2Seq\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1124\u001b[0m     ]\n\u001b[1;32m-> 1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   1126\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current model class (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is not compatible with `.generate()`, as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1127\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a language model head. Classes that support generation often end in one of these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1128\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mterminations_with_generation_support\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1129\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: The current model class (RobertaForMaskedLM) is not compatible with `.generate()`, as it doesn't have a language model head. Classes that support generation often end in one of these names: ['ForCausalLM', 'ForConditionalGeneration', 'ForSpeechSeq2Seq', 'ForVision2Seq']."
     ]
    }
   ],
   "source": [
    "documentation = generate_documentation(code_example, model, tokenizer)\n",
    "print(\"Generated Documentation:\", documentation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
