{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import RobertaForMaskedLM, Trainer, TrainingArguments\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'export' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HF_DATASETS_CACHE'] = './Datasets'\n",
    "!export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'> {'repository_name': 'ageitgey/face_recognition', 'func_path_in_repository': 'examples/face_recognition_knn.py', 'func_name': 'predict', 'whole_func_string': 'def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):\\n    \"\"\"\\n    Recognizes faces in given image using a trained KNN classifier\\n\\n    :param X_img_path: path to image to be recognized\\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\\n    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\\n    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\\n           of mis-classifying an unknown person as a known one.\\n    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\\n        For faces of unrecognized persons, the name \\'unknown\\' will be returned.\\n    \"\"\"\\n    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:\\n        raise Exception(\"Invalid image path: {}\".format(X_img_path))\\n\\n    if knn_clf is None and model_path is None:\\n        raise Exception(\"Must supply knn classifier either thourgh knn_clf or model_path\")\\n\\n    # Load a trained KNN model (if one was passed in)\\n    if knn_clf is None:\\n        with open(model_path, \\'rb\\') as f:\\n            knn_clf = pickle.load(f)\\n\\n    # Load image file and find face locations\\n    X_img = face_recognition.load_image_file(X_img_path)\\n    X_face_locations = face_recognition.face_locations(X_img)\\n\\n    # If no faces are found in the image, return an empty result.\\n    if len(X_face_locations) == 0:\\n        return []\\n\\n    # Find encodings for faces in the test iamge\\n    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations)\\n\\n    # Use the KNN model to find the best matches for the test face\\n    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)\\n    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]\\n\\n    # Predict classes and remove classifications that aren\\'t within the threshold\\n    return [(pred, loc) if rec else (\"unknown\", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]', 'language': 'python', 'func_code_string': 'def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):\\n    \"\"\"\\n    Recognizes faces in given image using a trained KNN classifier\\n\\n    :param X_img_path: path to image to be recognized\\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\\n    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\\n    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\\n           of mis-classifying an unknown person as a known one.\\n    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\\n        For faces of unrecognized persons, the name \\'unknown\\' will be returned.\\n    \"\"\"\\n    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:\\n        raise Exception(\"Invalid image path: {}\".format(X_img_path))\\n\\n    if knn_clf is None and model_path is None:\\n        raise Exception(\"Must supply knn classifier either thourgh knn_clf or model_path\")\\n\\n    # Load a trained KNN model (if one was passed in)\\n    if knn_clf is None:\\n        with open(model_path, \\'rb\\') as f:\\n            knn_clf = pickle.load(f)\\n\\n    # Load image file and find face locations\\n    X_img = face_recognition.load_image_file(X_img_path)\\n    X_face_locations = face_recognition.face_locations(X_img)\\n\\n    # If no faces are found in the image, return an empty result.\\n    if len(X_face_locations) == 0:\\n        return []\\n\\n    # Find encodings for faces in the test iamge\\n    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations)\\n\\n    # Use the KNN model to find the best matches for the test face\\n    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)\\n    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]\\n\\n    # Predict classes and remove classifications that aren\\'t within the threshold\\n    return [(pred, loc) if rec else (\"unknown\", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]', 'func_code_tokens': ['def', 'predict', '(', 'X_img_path', ',', 'knn_clf', '=', 'None', ',', 'model_path', '=', 'None', ',', 'distance_threshold', '=', '0.6', ')', ':', 'if', 'not', 'os', '.', 'path', '.', 'isfile', '(', 'X_img_path', ')', 'or', 'os', '.', 'path', '.', 'splitext', '(', 'X_img_path', ')', '[', '1', ']', '[', '1', ':', ']', 'not', 'in', 'ALLOWED_EXTENSIONS', ':', 'raise', 'Exception', '(', '\"Invalid image path: {}\"', '.', 'format', '(', 'X_img_path', ')', ')', 'if', 'knn_clf', 'is', 'None', 'and', 'model_path', 'is', 'None', ':', 'raise', 'Exception', '(', '\"Must supply knn classifier either thourgh knn_clf or model_path\"', ')', '# Load a trained KNN model (if one was passed in)', 'if', 'knn_clf', 'is', 'None', ':', 'with', 'open', '(', 'model_path', ',', \"'rb'\", ')', 'as', 'f', ':', 'knn_clf', '=', 'pickle', '.', 'load', '(', 'f', ')', '# Load image file and find face locations', 'X_img', '=', 'face_recognition', '.', 'load_image_file', '(', 'X_img_path', ')', 'X_face_locations', '=', 'face_recognition', '.', 'face_locations', '(', 'X_img', ')', '# If no faces are found in the image, return an empty result.', 'if', 'len', '(', 'X_face_locations', ')', '==', '0', ':', 'return', '[', ']', '# Find encodings for faces in the test iamge', 'faces_encodings', '=', 'face_recognition', '.', 'face_encodings', '(', 'X_img', ',', 'known_face_locations', '=', 'X_face_locations', ')', '# Use the KNN model to find the best matches for the test face', 'closest_distances', '=', 'knn_clf', '.', 'kneighbors', '(', 'faces_encodings', ',', 'n_neighbors', '=', '1', ')', 'are_matches', '=', '[', 'closest_distances', '[', '0', ']', '[', 'i', ']', '[', '0', ']', '<=', 'distance_threshold', 'for', 'i', 'in', 'range', '(', 'len', '(', 'X_face_locations', ')', ')', ']', \"# Predict classes and remove classifications that aren't within the threshold\", 'return', '[', '(', 'pred', ',', 'loc', ')', 'if', 'rec', 'else', '(', '\"unknown\"', ',', 'loc', ')', 'for', 'pred', ',', 'loc', ',', 'rec', 'in', 'zip', '(', 'knn_clf', '.', 'predict', '(', 'faces_encodings', ')', ',', 'X_face_locations', ',', 'are_matches', ')', ']'], 'func_documentation_string': \"Recognizes faces in given image using a trained KNN classifier\\n\\n    :param X_img_path: path to image to be recognized\\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\\n    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\\n    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\\n           of mis-classifying an unknown person as a known one.\\n    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\\n        For faces of unrecognized persons, the name 'unknown' will be returned.\", 'func_documentation_tokens': ['Recognizes', 'faces', 'in', 'given', 'image', 'using', 'a', 'trained', 'KNN', 'classifier'], 'split_name': 'train', 'func_code_url': 'https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L111-L150'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Load the CodeSearchNet datasets for Python and Java\n",
    "train_python = load_dataset('code_search_net', 'python', trust_remote_code=True, cache_dir='./Datasets', split='train')\n",
    "test_python = load_dataset('code_search_net', 'python', trust_remote_code=True, cache_dir='./Datasets', split='test')\n",
    "val_python = load_dataset('code_search_net', 'python', trust_remote_code=True, cache_dir='./Datasets', split='validation')\n",
    "\n",
    "# dataset_javascript = load_dataset('code_search_net', 'javascript', trust_remote_code=True)\n",
    "print(type(train_python), train_python[1])\n",
    "# Combine the training sets of the datasets for multiple programming language\n",
    "train_dataset = train_python.shuffle(seed=42).select(range(2000))\n",
    "print(type(train_dataset))\n",
    "# combined_dataset = [combined_dataset, dataset_javascript['train']])\n",
    "\n",
    "test_dataset = dataset_python['test'].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# Sample for validation\n",
    "val_dataset = dataset_python['validation'].shuffle(seed=42).select(range(500))\n",
    "\n",
    "# Display the first example from the combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((412178, 11),\n",
       " (22176, 11),\n",
       " (23107, 11),\n",
       " datasets.arrow_dataset.Dataset,\n",
       " datasets.arrow_dataset.Dataset,\n",
       " datasets.arrow_dataset.Dataset,\n",
       " dict)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape,test_dataset.shape, val_dataset.shape, type(train_dataset), type(val_dataset), type(test_dataset), type(train_dataset[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:3274: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "token = \"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\"\n",
    "model = RobertaForMaskedLM.from_pretrained(\"microsoft/codebert-base\", use_auth_token=token, cache_dir = \"./Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda'), 'NVIDIA GeForce GTX 1650')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "device, torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for your model (e.g., CodeBERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['func_code_string'],  # Adjust the key based on your dataset structure\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=False,\n",
    "     fp16=True # Set this to False to avoid the column check\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    input_encodings = tokenizer(\n",
    "        examples['func_code_string'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        examples['func_documentation_string'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_encodings['input_ids'],  # Should be list of integers\n",
    "        'attention_mask': input_encodings['attention_mask'],  # Should be list of integers\n",
    "        'labels': labels['input_ids'],  # Should be list of integers\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "    num_rows: 412178\n",
      "})\n",
      "{'repository_name': 'ageitgey/face_recognition', 'func_path_in_repository': 'examples/face_recognition_knn.py', 'func_name': 'train', 'whole_func_string': 'def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo=\\'ball_tree\\', verbose=False):\\n    \"\"\"\\n    Trains a k-nearest neighbors classifier for face recognition.\\n\\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\\n\\n     (View in source code to see train_dir example tree structure)\\n\\n     Structure:\\n        <train_dir>/\\n        â”œâ”€â”€ <person1>/\\n        â”‚   â”œâ”€â”€ <somename1>.jpeg\\n        â”‚   â”œâ”€â”€ <somename2>.jpeg\\n        â”‚   â”œâ”€â”€ ...\\n        â”œâ”€â”€ <person2>/\\n        â”‚   â”œâ”€â”€ <somename1>.jpeg\\n        â”‚   â””â”€â”€ <somename2>.jpeg\\n        â””â”€â”€ ...\\n\\n    :param model_save_path: (optional) path to save model on disk\\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\\n    :param verbose: verbosity of training\\n    :return: returns knn classifier that was trained on the given data.\\n    \"\"\"\\n    X = []\\n    y = []\\n\\n    # Loop through each person in the training set\\n    for class_dir in os.listdir(train_dir):\\n        if not os.path.isdir(os.path.join(train_dir, class_dir)):\\n            continue\\n\\n        # Loop through each training image for the current person\\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\\n            image = face_recognition.load_image_file(img_path)\\n            face_bounding_boxes = face_recognition.face_locations(image)\\n\\n            if len(face_bounding_boxes) != 1:\\n                # If there are no people (or too many people) in a training image, skip the image.\\n                if verbose:\\n                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn\\'t find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\\n            else:\\n                # Add face encoding for current image to the training set\\n                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\\n                y.append(class_dir)\\n\\n    # Determine how many neighbors to use for weighting in the KNN classifier\\n    if n_neighbors is None:\\n        n_neighbors = int(round(math.sqrt(len(X))))\\n        if verbose:\\n            print(\"Chose n_neighbors automatically:\", n_neighbors)\\n\\n    # Create and train the KNN classifier\\n    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights=\\'distance\\')\\n    knn_clf.fit(X, y)\\n\\n    # Save the trained KNN classifier\\n    if model_save_path is not None:\\n        with open(model_save_path, \\'wb\\') as f:\\n            pickle.dump(knn_clf, f)\\n\\n    return knn_clf', 'language': 'python', 'func_code_string': 'def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo=\\'ball_tree\\', verbose=False):\\n    \"\"\"\\n    Trains a k-nearest neighbors classifier for face recognition.\\n\\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\\n\\n     (View in source code to see train_dir example tree structure)\\n\\n     Structure:\\n        <train_dir>/\\n        â”œâ”€â”€ <person1>/\\n        â”‚   â”œâ”€â”€ <somename1>.jpeg\\n        â”‚   â”œâ”€â”€ <somename2>.jpeg\\n        â”‚   â”œâ”€â”€ ...\\n        â”œâ”€â”€ <person2>/\\n        â”‚   â”œâ”€â”€ <somename1>.jpeg\\n        â”‚   â””â”€â”€ <somename2>.jpeg\\n        â””â”€â”€ ...\\n\\n    :param model_save_path: (optional) path to save model on disk\\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\\n    :param verbose: verbosity of training\\n    :return: returns knn classifier that was trained on the given data.\\n    \"\"\"\\n    X = []\\n    y = []\\n\\n    # Loop through each person in the training set\\n    for class_dir in os.listdir(train_dir):\\n        if not os.path.isdir(os.path.join(train_dir, class_dir)):\\n            continue\\n\\n        # Loop through each training image for the current person\\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\\n            image = face_recognition.load_image_file(img_path)\\n            face_bounding_boxes = face_recognition.face_locations(image)\\n\\n            if len(face_bounding_boxes) != 1:\\n                # If there are no people (or too many people) in a training image, skip the image.\\n                if verbose:\\n                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn\\'t find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\\n            else:\\n                # Add face encoding for current image to the training set\\n                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\\n                y.append(class_dir)\\n\\n    # Determine how many neighbors to use for weighting in the KNN classifier\\n    if n_neighbors is None:\\n        n_neighbors = int(round(math.sqrt(len(X))))\\n        if verbose:\\n            print(\"Chose n_neighbors automatically:\", n_neighbors)\\n\\n    # Create and train the KNN classifier\\n    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights=\\'distance\\')\\n    knn_clf.fit(X, y)\\n\\n    # Save the trained KNN classifier\\n    if model_save_path is not None:\\n        with open(model_save_path, \\'wb\\') as f:\\n            pickle.dump(knn_clf, f)\\n\\n    return knn_clf', 'func_code_tokens': ['def', 'train', '(', 'train_dir', ',', 'model_save_path', '=', 'None', ',', 'n_neighbors', '=', 'None', ',', 'knn_algo', '=', \"'ball_tree'\", ',', 'verbose', '=', 'False', ')', ':', 'X', '=', '[', ']', 'y', '=', '[', ']', '# Loop through each person in the training set', 'for', 'class_dir', 'in', 'os', '.', 'listdir', '(', 'train_dir', ')', ':', 'if', 'not', 'os', '.', 'path', '.', 'isdir', '(', 'os', '.', 'path', '.', 'join', '(', 'train_dir', ',', 'class_dir', ')', ')', ':', 'continue', '# Loop through each training image for the current person', 'for', 'img_path', 'in', 'image_files_in_folder', '(', 'os', '.', 'path', '.', 'join', '(', 'train_dir', ',', 'class_dir', ')', ')', ':', 'image', '=', 'face_recognition', '.', 'load_image_file', '(', 'img_path', ')', 'face_bounding_boxes', '=', 'face_recognition', '.', 'face_locations', '(', 'image', ')', 'if', 'len', '(', 'face_bounding_boxes', ')', '!=', '1', ':', '# If there are no people (or too many people) in a training image, skip the image.', 'if', 'verbose', ':', 'print', '(', '\"Image {} not suitable for training: {}\"', '.', 'format', '(', 'img_path', ',', '\"Didn\\'t find a face\"', 'if', 'len', '(', 'face_bounding_boxes', ')', '<', '1', 'else', '\"Found more than one face\"', ')', ')', 'else', ':', '# Add face encoding for current image to the training set', 'X', '.', 'append', '(', 'face_recognition', '.', 'face_encodings', '(', 'image', ',', 'known_face_locations', '=', 'face_bounding_boxes', ')', '[', '0', ']', ')', 'y', '.', 'append', '(', 'class_dir', ')', '# Determine how many neighbors to use for weighting in the KNN classifier', 'if', 'n_neighbors', 'is', 'None', ':', 'n_neighbors', '=', 'int', '(', 'round', '(', 'math', '.', 'sqrt', '(', 'len', '(', 'X', ')', ')', ')', ')', 'if', 'verbose', ':', 'print', '(', '\"Chose n_neighbors automatically:\"', ',', 'n_neighbors', ')', '# Create and train the KNN classifier', 'knn_clf', '=', 'neighbors', '.', 'KNeighborsClassifier', '(', 'n_neighbors', '=', 'n_neighbors', ',', 'algorithm', '=', 'knn_algo', ',', 'weights', '=', \"'distance'\", ')', 'knn_clf', '.', 'fit', '(', 'X', ',', 'y', ')', '# Save the trained KNN classifier', 'if', 'model_save_path', 'is', 'not', 'None', ':', 'with', 'open', '(', 'model_save_path', ',', \"'wb'\", ')', 'as', 'f', ':', 'pickle', '.', 'dump', '(', 'knn_clf', ',', 'f', ')', 'return', 'knn_clf'], 'func_documentation_string': 'Trains a k-nearest neighbors classifier for face recognition.\\n\\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\\n\\n     (View in source code to see train_dir example tree structure)\\n\\n     Structure:\\n        <train_dir>/\\n        â”œâ”€â”€ <person1>/\\n        â”‚   â”œâ”€â”€ <somename1>.jpeg\\n        â”‚   â”œâ”€â”€ <somename2>.jpeg\\n        â”‚   â”œâ”€â”€ ...\\n        â”œâ”€â”€ <person2>/\\n        â”‚   â”œâ”€â”€ <somename1>.jpeg\\n        â”‚   â””â”€â”€ <somename2>.jpeg\\n        â””â”€â”€ ...\\n\\n    :param model_save_path: (optional) path to save model on disk\\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\\n    :param verbose: verbosity of training\\n    :return: returns knn classifier that was trained on the given data.', 'func_documentation_tokens': ['Trains', 'a', 'k', '-', 'nearest', 'neighbors', 'classifier', 'for', 'face', 'recognition', '.'], 'split_name': 'train', 'func_code_url': 'https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L46-L108'}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_dataset)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_dataset[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m----> 4\u001b[0m tokenized_train_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m tokenized_val_data \u001b[38;5;241m=\u001b[39m val_dataset[:\u001b[38;5;241m1200\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenized_train_data)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "# Tokenize the datasets\n",
    "print(train_dataset)\n",
    "print(train_dataset[0])\n",
    "tokenized_train_data = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_data = val_dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_train_data)\n",
    "print(tokenized_train_data[0])\n",
    "# Remove unused columns if necessary\n",
    "tokenized_train_data = tokenized_train_data.remove_columns(train_dataset.column_names)\n",
    "tokenized_val_data = tokenized_val_data.remove_columns(val_dataset.column_names)\n",
    "print(tokenized_train_data)\n",
    "print(tokenized_train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Extract the necessary components from each item in the batch\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Convert lists of lists into tensors\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids).to(device),\n",
    "        'attention_mask': torch.tensor(attention_mask).to(device),\n",
    "        'labels': torch.tensor(labels).to(device),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the DataLoader with the custom collate function\n",
    "train_dataloader = DataLoader(tokenized_train_data, batch_size=16, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[0, 9232, 2341, 1640, 21714, 1215, 41292, 6, 1421, 1215, 31575, 1215, 22609, 5214, 29802, 6, 295, 1215, 858, 8774, 19357, 5214, 29802, 6, 11269, 282, 1215, 337, 2977, 47579, 3512, 1215, 21512, 3934, 33760, 3876, 5214, 46659, 3256, 50118, 1437, 1437, 1437, 49434, 50118, 1437, 1437, 1437, 2393, 5069, 10, 449, 12, 858, 18759, 6611, 1380, 24072, 13, 652, 4972, 4, 50140, 1437, 1437, 1437, 4832, 46669, 2341, 1215, 41292, 35, 31826, 14, 6308, 10, 2849, 12, 48626, 13, 349, 684, 621, 6, 19, 63, 766, 4, 50140, 1437, 1437, 1437, 1437, 36, 22130, 11, 1300, 3260, 7, 192, 2341, 1215, 41292, 1246, 3907, 3184, 43, 50140, 1437, 1437, 1437, 1437, 32732, 35, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 28696, 21714, 1215, 41292, 15698, 73, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 49450, 28696, 5970, 134, 15698, 73, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 48925, 1437, 1437, 49450, 28696, 29, 14900, 4344, 134, 48691, 267, 41191, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 48925, 1437, 1437, 49450, 28696, 29, 14900, 4344, 176, 48691, 267, 41191, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 48925, 1437, 1437, 49450, 1666, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 49450, 28696, 5970, 176, 15698, 73, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 48925, 1437, 1437, 49450, 28696, 29, 14900, 4344, 134, 48691, 267, 41191, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 48925, 1437, 1437, 46277, 10674, 49546, 28696, 29, 14900, 4344, 176, 48691, 267, 41191, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 46277, 10674, 49546, 1666, 50140, 1437, 1437, 1437, 4832, 46669, 1421, 1215, 31575, 1215, 22609, 35, 36, 42029, 43, 2718, 7, 1871, 1421, 15, 21675, 50118, 1437, 1437, 1437, 4832, 46669, 295, 1215, 858, 8774, 19357, 35, 36, 42029, 43, 346, 9, 6611, 7, 9832, 11, 20257, 4, 43936, 6885, 114, 45, 17966, 50118, 1437, 1437, 1437, 4832, 46669, 11269, 282, 1215, 337, 2977, 35, 36, 42029, 43, 7482, 414, 3184, 7, 323, 11269, 282, 4, 43234, 16, 1011, 1215, 21512, 50118, 1437, 1437, 1437, 4832, 46669, 33760, 3876, 35, 33760, 39533, 9, 1058, 50118, 1437, 1437, 1437, 4832, 30921, 35, 2886, 11269, 282, 1380, 24072, 14, 21, 5389, 15, 5, 576, 414, 4, 50118, 1437, 1437, 1437, 49434, 50118, 1437, 1437, 1437, 1577, 5457, 48081, 50118, 1437, 1437, 1437, 1423, 5457, 48081, 50140, 1437, 1437, 1437, 849, 23005, 149, 349, 621, 11, 5, 1058, 278, 50118, 1437, 1437, 1437, 13, 1380, 1215, 41292, 11, 11988, 4, 8458, 41292, 1640, 21714, 1215, 41292, 3256, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 114, 45, 11988, 4, 22609, 4, 29306, 853, 1640, 366, 4, 22609, 4, 26960, 1640, 21714, 1215, 41292, 6, 1380, 1215, 41292, 43, 3256, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 535, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 849, 23005, 149, 349, 1058, 2274, 13, 5, 595, 621, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 13, 48599, 1215, 22609, 11, 2274, 1215, 42018, 1215, 179, 1215, 48811, 1640, 366, 4, 22609, 4, 2], [0, 9232, 7006, 1640, 1000, 1215, 34252, 1215, 22609, 6, 11269, 282, 1215, 3998, 506, 5214, 29802, 6, 1421, 1215, 22609, 5214, 29802, 6, 4472, 1215, 212, 45749, 5214, 288, 4, 401, 3256, 50118, 1437, 1437, 1437, 49434, 50118, 1437, 1437, 1437, 23288, 7396, 2419, 11, 576, 2274, 634, 10, 5389, 229, 20057, 1380, 24072, 50140, 1437, 1437, 1437, 4832, 46669, 1577, 1215, 34252, 1215, 22609, 35, 2718, 7, 2274, 7, 28, 4984, 50118, 1437, 1437, 1437, 4832, 46669, 11269, 282, 1215, 3998, 506, 35, 36, 42029, 43, 10, 11269, 282, 1380, 24072, 7626, 4, 114, 45, 17966, 6, 1421, 1215, 31575, 1215, 22609, 531, 28, 17966, 4, 50118, 1437, 1437, 1437, 4832, 46669, 1421, 1215, 22609, 35, 36, 42029, 43, 2718, 7, 10, 1339, 1329, 11269, 282, 1380, 24072, 4, 114, 45, 17966, 6, 1421, 1215, 31575, 1215, 22609, 531, 28, 11269, 282, 1215, 3998, 506, 4, 50118, 1437, 1437, 1437, 4832, 46669, 4472, 1215, 212, 45749, 35, 36, 42029, 43, 4472, 11543, 13, 652, 20257, 4, 5, 2514, 24, 16, 6, 5, 55, 778, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 9, 3834, 12, 4684, 4945, 41, 4727, 621, 25, 10, 684, 65, 4, 50118, 1437, 1437, 1437, 4832, 30921, 35, 10, 889, 9, 2523, 8, 652, 3237, 13, 5, 4984, 2419, 11, 5, 2274, 35, 48794, 13650, 6, 8191, 154, 2233, 238, 1666, 8174, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 286, 2419, 9, 35152, 1538, 5151, 6, 5, 766, 128, 42230, 108, 40, 28, 1835, 4, 50118, 1437, 1437, 1437, 49434, 50118, 1437, 1437, 1437, 114, 45, 11988, 4, 22609, 4, 32062, 1848, 1640, 1000, 1215, 34252, 1215, 22609, 43, 50, 11988, 4, 22609, 4, 34282, 1459, 11483, 1640, 1000, 1215, 34252, 1215, 22609, 48462, 134, 46386, 134, 48937, 45, 11, 12389, 4581, 1691, 1215, 42680, 12743, 11654, 35, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1693, 47617, 46469, 49695, 2274, 2718, 35, 49153, 845, 34609, 1640, 1000, 1215, 34252, 1215, 22609, 35122, 50140, 1437, 1437, 1437, 114, 11269, 282, 1215, 3998, 506, 16, 9291, 8, 1421, 1215, 22609, 16, 9291, 35, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1693, 47617, 46469, 41033, 1787, 11269, 282, 1380, 24072, 1169, 3553, 2126, 4147, 11269, 282, 1215, 3998, 506, 50, 1421, 1215, 22609, 8070, 50140, 1437, 1437, 1437, 849, 38355, 10, 5389, 229, 20057, 1421, 36, 1594, 65, 21, 1595, 11, 43, 50118, 1437, 1437, 1437, 114, 11269, 282, 1215, 3998, 506, 16, 9291, 35, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 19, 490, 1640, 21818, 1215, 22609, 6, 128, 20815, 27645, 25, 856, 35, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 11269, 282, 1215, 3998, 506, 5457, 1339, 459, 4, 16204, 1640, 506, 43, 50140, 1437, 1437, 1437, 849, 38355, 2274, 2870, 8, 465, 652, 3237, 50118, 1437, 1437, 1437, 1577, 1215, 34252, 5457, 652, 1215, 28039, 7469, 4, 16204, 1215, 20094, 1215, 21710, 1640, 1000, 1215, 34252, 1215, 22609, 43, 50118, 1437, 1437, 1437, 1577, 1215, 9021, 1215, 26516, 1635, 5457, 652, 1215, 28039, 2], [0, 9232, 311, 1215, 37466, 26579, 1215, 31479, 2507, 1215, 261, 1215, 20094, 1640, 34252, 1215, 22609, 6, 12535, 3256, 50118, 1437, 1437, 1437, 49434, 50118, 1437, 1437, 1437, 28677, 5, 652, 4972, 775, 21545, 4, 50140, 1437, 1437, 1437, 4832, 46669, 48599, 1215, 22609, 35, 2718, 7, 2274, 7, 28, 4984, 50118, 1437, 1437, 1437, 4832, 46669, 12535, 35, 775, 9, 5, 7006, 5043, 50118, 1437, 1437, 1437, 4832, 30921, 35, 50118, 1437, 1437, 1437, 49434, 50118, 1437, 1437, 1437, 25172, 1215, 20094, 5457, 2960, 4, 12592, 1640, 34252, 1215, 22609, 322, 3865, 9942, 46469, 46647, 8070, 50118, 1437, 1437, 1437, 2451, 5457, 2960, 45941, 4, 45941, 1640, 642, 718, 1215, 20094, 43, 50140, 1437, 1437, 1437, 13, 766, 6, 36, 8766, 6, 235, 6, 2576, 6, 314, 43, 11, 12535, 35, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 849, 29109, 10, 2233, 198, 5, 652, 634, 5, 24722, 1722, 20686, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 2451, 4, 41087, 14982, 48461, 1640, 6960, 6, 299, 238, 36, 4070, 6, 2576, 46934, 17475, 49487, 288, 6, 321, 6, 28080, 35122, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 849, 345, 18, 10, 13673, 11, 24722, 1722, 147, 24, 19250, 62, 19, 786, 12, 44987, 12, 398, 2788, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 849, 77, 634, 5, 6814, 828, 32557, 28716, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 766, 5457, 766, 4, 225, 20414, 46469, 44987, 12, 398, 8070, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 849, 29109, 10, 6929, 19, 10, 766, 874, 5, 652, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 2788, 1215, 36097, 6, 2788, 1215, 37009, 5457, 2451, 4, 29015, 10799, 1640, 13650, 43, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 2451, 4, 41087, 14982, 48461, 1640, 6960, 6, 2576, 111, 2788, 1215, 37009, 111, 158, 238, 36, 4070, 6, 2576, 46934, 3300, 49487, 288, 6, 321, 6, 28080, 238, 17475, 49487, 288, 6, 321, 6, 28080, 35122, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 2451, 4, 29015, 48461, 6960, 2055, 231, 6, 2576, 111, 2788, 1215, 37009, 111, 195, 238, 766, 6, 3300, 49487, 22260, 6, 28080, 6, 28080, 6, 28080, 35122, 50140, 1437, 1437, 1437, 849, 27336, 5, 5523, 5560, 31, 3783, 25, 228, 5, 24722, 1722, 41825, 50118, 1437, 1437, 1437, 2424, 2451, 50140, 1437, 1437, 1437, 849, 17091, 5, 5203, 2274, 50118, 1437, 1437, 1437, 25172, 1215, 20094, 4, 12005, 43048, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[0, 12667, 5069, 10, 449, 12, 858, 18759, 6611, 1380, 24072, 13, 652, 4972, 4, 50140, 1437, 1437, 1437, 4832, 46669, 2341, 1215, 41292, 35, 31826, 14, 6308, 10, 2849, 12, 48626, 13, 349, 684, 621, 6, 19, 63, 766, 4, 50140, 1437, 1437, 1437, 1437, 36, 22130, 11, 1300, 3260, 7, 192, 2341, 1215, 41292, 1246, 3907, 3184, 43, 50140, 1437, 1437, 1437, 1437, 32732, 35, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 28696, 21714, 1215, 41292, 15698, 73, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 49450, 28696, 5970, 134, 15698, 73, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 48925, 1437, 1437, 49450, 28696, 29, 14900, 4344, 134, 48691, 267, 41191, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 48925, 1437, 1437, 49450, 28696, 29, 14900, 4344, 176, 48691, 267, 41191, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 48925, 1437, 1437, 49450, 1666, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 49450, 28696, 5970, 176, 15698, 73, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 48925, 1437, 1437, 49450, 28696, 29, 14900, 4344, 134, 48691, 267, 41191, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 48925, 1437, 1437, 46277, 10674, 49546, 28696, 29, 14900, 4344, 176, 48691, 267, 41191, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 46277, 10674, 49546, 1666, 50140, 1437, 1437, 1437, 4832, 46669, 1421, 1215, 31575, 1215, 22609, 35, 36, 42029, 43, 2718, 7, 1871, 1421, 15, 21675, 50118, 1437, 1437, 1437, 4832, 46669, 295, 1215, 858, 8774, 19357, 35, 36, 42029, 43, 346, 9, 6611, 7, 9832, 11, 20257, 4, 43936, 6885, 114, 45, 17966, 50118, 1437, 1437, 1437, 4832, 46669, 11269, 282, 1215, 337, 2977, 35, 36, 42029, 43, 7482, 414, 3184, 7, 323, 11269, 282, 4, 43234, 16, 1011, 1215, 21512, 50118, 1437, 1437, 1437, 4832, 46669, 33760, 3876, 35, 33760, 39533, 9, 1058, 50118, 1437, 1437, 1437, 4832, 30921, 35, 2886, 11269, 282, 1380, 24072, 14, 21, 5389, 15, 5, 576, 414, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 21109, 18857, 7396, 2419, 11, 576, 2274, 634, 10, 5389, 229, 20057, 1380, 24072, 50140, 1437, 1437, 1437, 4832, 46669, 1577, 1215, 34252, 1215, 22609, 35, 2718, 7, 2274, 7, 28, 4984, 50118, 1437, 1437, 1437, 4832, 46669, 11269, 282, 1215, 3998, 506, 35, 36, 42029, 43, 10, 11269, 282, 1380, 24072, 7626, 4, 114, 45, 17966, 6, 1421, 1215, 31575, 1215, 22609, 531, 28, 17966, 4, 50118, 1437, 1437, 1437, 4832, 46669, 1421, 1215, 22609, 35, 36, 42029, 43, 2718, 7, 10, 1339, 1329, 11269, 282, 1380, 24072, 4, 114, 45, 17966, 6, 1421, 1215, 31575, 1215, 22609, 531, 28, 11269, 282, 1215, 3998, 506, 4, 50118, 1437, 1437, 1437, 4832, 46669, 4472, 1215, 212, 45749, 35, 36, 42029, 43, 4472, 11543, 13, 652, 20257, 4, 5, 2514, 24, 16, 6, 5, 55, 778, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 9, 3834, 12, 4684, 4945, 41, 4727, 621, 25, 10, 684, 65, 4, 50118, 1437, 1437, 1437, 4832, 30921, 35, 10, 889, 9, 2523, 8, 652, 3237, 13, 5, 4984, 2419, 11, 5, 2274, 35, 48794, 13650, 6, 8191, 154, 2233, 238, 1666, 8174, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 286, 2419, 9, 35152, 1538, 5151, 6, 5, 766, 128, 42230, 108, 40, 28, 1835, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 3609, 2176, 5, 652, 4972, 775, 21545, 4, 50140, 1437, 1437, 1437, 4832, 46669, 48599, 1215, 22609, 35, 2718, 7, 2274, 7, 28, 4984, 50118, 1437, 1437, 1437, 4832, 46669, 12535, 35, 775, 9, 5, 7006, 5043, 50118, 1437, 1437, 1437, 4832, 30921, 35, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# Check the first few examples in your tokenized data\n",
    "print(tokenized_train_data[:3])  # Adjust slicing based on your data structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.data = tokenized_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Wrap your tokenized data into a Dataset\n",
    "train_dataset = CustomDataset(tokenized_train_data)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\accelerator.py:463: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Use this Dataset with the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,  # Your pre-initialized model\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=train_dataset,  # Use the custom Dataset\n",
    "    eval_dataset=val_dataset,  # Optional evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19320 [00:00<?, ?it/s]c:\\Users\\sainithin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:370: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "  0%|          | 19/19320 [28:51<489:11:47, 91.24s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 866629/866629 [03:34<00:00, 4035.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the datasets\n",
    "tokenized_train_data = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_data = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove the original code strings if necessary\n",
    "tokenized_train_data = tokenized_train_data.remove_columns(['func_code_string'])\n",
    "tokenized_val_data = tokenized_val_data.remove_columns(['func_code_string'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the input code and the corresponding documentation\n",
    "    input_encodings = tokenizer(\n",
    "        examples['func_code_string'],  # Adjust the key based on your dataset structure\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        examples['func_documentation_string'],  # Adjust this to the correct documentation key\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    # Return input encodings and labels\n",
    "    return {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': labels['input_ids'],  # This is how the model expects labels\n",
    "    }\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train_data = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_data = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove unused columns if necessary\n",
    "tokenized_train_data = tokenized_train_data.remove_columns(train_dataset.column_names)\n",
    "tokenized_val_data = tokenized_val_data.remove_columns(val_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Assuming all inputs are lists of same length\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids),\n",
    "        'attention_mask': torch.tensor(attention_mask),\n",
    "        'labels': torch.tensor(labels),\n",
    "    }\n",
    "\n",
    "# Use this in your DataLoader\n",
    "train_dataloader = DataLoader(tokenized_train_data, batch_size=16, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_python['train'].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "token = \"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\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-multi\", use_auth_token=token, cache_dir = \"./Models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Tokenize the datasets using the correct field name\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['func_code_string'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = combined_dataset.map(tokenize_function, batched=True)\n",
    "# After tokenizing, save the dataset to a specific location\n",
    "output_directory = './Datasets/tokenized'\n",
    "tokenized_datasets.save_to_disk(output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a preprocessing function to tokenize your dataset\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['input_text']  # Replace with the correct key for input texts\n",
    "    targets = examples['target_text']  # Replace with the correct key for target texts\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "    \n",
    "    # Tokenize the targets (for seq2seq models like T5)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=512, truncation=True)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing to your dataset\n",
    "tokenized_datasets = combined_dataset.map(tokenize_function, batched=True)\n",
    "# After tokenizing, save the dataset to a specific location\n",
    "output_directory = './Datasets/tokenized'\n",
    "tokenized_datasets.save_to_disk(output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization\n",
    "tokenized_datasets_validation = validation_dataset.map(tokenize_function, batched=True)\n",
    "# After tokenizing, save the dataset to a specific location\n",
    "output_directory = './Datasets/tokenized/validation'\n",
    "tokenized_datasets_validation.save_to_disk(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSeq2SeqLM\n",
    "# Load your pre-trained model (e.g., T5 or CodeBERT)\n",
    "model_name = \"t5-base\"  # or another model suitable for code-to-text tasks\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                  # output directory for model predictions and checkpoints\n",
    "    evaluation_strategy=\"steps\",             # evaluation is done at the end of each epoch\n",
    "    learning_rate=5e-5,                      # learning rate\n",
    "    per_device_train_batch_size=3,           # batch size for training\n",
    "    per_device_eval_batch_size=3,            # batch size for evaluation\n",
    "    num_train_epochs=3,                      # total number of training epochs\n",
    "    weight_decay=0.01,                       # strength of weight decay\n",
    "    logging_dir='./logs',                    # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,             # load the best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\"        # metric for determining the best model\n",
    ")\n",
    "\n",
    "# Function to compute metrics\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions.argmax(axis=-1)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels_ids = np.where(labels_ids != -100, labels_ids, tokenizer.pad_token_id)\n",
    "    # Decode the predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Use a metric such as BLEU or ROUGE here\n",
    "    # For simplicity, you can use the following placeholder:\n",
    "    return {\"bleu\": calculate_bleu(decoded_preds, decoded_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets)  # to check the overall structure\n",
    "print(tokenized_datasets.column_names)  # to list all column names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")  # Change to \"cuda\" when you want to run on GPU\n",
    "#model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# Custom data collator\n",
    "def custom_data_collator(features):\n",
    "    if not isinstance(features[0], dict):\n",
    "        # Ensure that features are converted to dicts\n",
    "        features = [vars(f) if hasattr(f, '__dict__') else f for f in features]\n",
    "    return DataCollatorForSeq2Seq(tokenizer)(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"func_code_string\"],\n",
    "    eval_dataset=tokenized_datasets_validation[\"func_code_string\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=custom_data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to prepare the dataset\n",
    "def prepare_data(example):\n",
    "    return {\n",
    "        'input_ids': example['func_code_tokens'],  # Tokenized function code\n",
    "        'attention_mask': [1] * len(example['func_code_tokens']),  # All tokens are valid\n",
    "        'labels': example['func_documentation_tokens']  # Documentation tokens\n",
    "    }\n",
    "\n",
    "# Map the Python dataset\n",
    "train_data_python = dataset_python['train'].map(prepare_data, remove_columns=dataset_python['train'].column_names)\n",
    "val_data_python = dataset_python['validation'].map(prepare_data, remove_columns=dataset_python['validation'].column_names)\n",
    "\n",
    "# Map the Java dataset\n",
    "train_data_java = dataset_java['train'].map(prepare_data, remove_columns=dataset_java['train'].column_names)\n",
    "val_data_java = dataset_java['validation'].map(prepare_data, remove_columns=dataset_java['validation'].column_names)\n",
    "\n",
    "# Check the mapped dataset columns for Python and Java\n",
    "print(\"Mapped Train columns (Python):\", train_data_python.column_names)\n",
    "print(\"Mapped Validation columns (Python):\", val_data_python.column_names)\n",
    "print(\"Mapped Train columns (Java):\", train_data_java.column_names)\n",
    "print(\"Mapped Validation columns (Java):\", val_data_java.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Tokenization function\n",
    "def prepare_data(example):\n",
    "    # Tokenizing code and documentation\n",
    "    code_encoding = tokenizer(example['func_code_tokens'], truncation=True, padding='max_length', max_length=512)\n",
    "    doc_encoding = tokenizer(example['func_documentation_tokens'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "    return {\n",
    "        'input_ids': code_encoding['input_ids'],  # Tokenized function code\n",
    "        'attention_mask': code_encoding['attention_mask'],  # Attention mask\n",
    "        'labels': doc_encoding['input_ids']  # Tokenized documentation\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('./model')\n",
    "tokenizer.save_pretrained('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets['func_code_string'][:10])  # Check first 10 samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
